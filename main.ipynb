{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Hashing Demo\n",
    "\n",
    "## Description\n",
    "\n",
    "Initially, the objective is to identify relationships among various types of texts - those that are similar, those that are semantically similar (generated using Large Language Models, or LLMs) and those that are completely unrelated. For this purpose, the \"Amazon Food Reviews\" dataset from Kaggle is utilized in this notebook.\n",
    "\n",
    "Subsequently, texts are generated using [Marvin](https://www.askmarvin.ai/) through their `marvin` Python package. The dataset includes paragraph-sized texts and their semantically similar counterparts. Correspondingly, matrices are created and visualized as heatmap plots to demonstrate the semantic relationships, with an increasing number of hyperplanes.\n",
    "\n",
    "From an algorithmic standpoint, the Locality Sensitive Hashing (LSH) method is employed. This technique facilitates the creation of hyperplanes between different texts (e.g., food reviews), which are represented as embedding vectors.\n",
    "> The size of the embedding vector is 1536 for OpenAI's small embedding model, and 3072 for the large model.\n",
    "\n",
    "Subsequently, for any given text, a semantic hash is computed. This process involves converting a large embedding vector (a numerical representation of text) into a few bits (representing the number of hyperplanes), akin to a hash code. For example, the phrase \"The food was very delicious\" could be represented as `1011` in a system with 4 hyperplanes. Finally, texts with identical hashes are grouped into multiple buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import csv\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "All the parameters (constants mostly) are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the input data for the semantic hashing demo.\n",
    "\"\"\"\n",
    "\n",
    "# no. of hyperplanes\n",
    "nbits = 8\n",
    "\n",
    "\n",
    "# data file\n",
    "# NOTE: This data file has around 570k text reviews (of types: single line, paragraph).\n",
    "# So, parse accordingly depending on the computational resources for bucketing.\n",
    "data_file = \"./data/fine_food_reviews_1k.csv\"\n",
    "preprocessed_data_file = \"./output/preprocessed_data.csv\"\n",
    "generated_data_file = \"./data/paragraphs.csv\"\n",
    "\n",
    "# no. of text samples\n",
    "n = 20\n",
    "\n",
    "# seed for hyperplane generation\n",
    "seed = 2254  # subspace address format prefix\n",
    "\n",
    "# embedding model\n",
    "model = \"text-embedding-3-small\"\n",
    "# model = \"text-embedding-3-large\"\n",
    "\n",
    "# embedding size\n",
    "embedding_size = int(1536)  # for small\n",
    "# embedding_size = int(3072)   # for large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_file_exists(dir_name: str, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Create the directory and the file if it doesn't exist.\n",
    "    The function is efficient as it avoids redundant directory and file creation.\n",
    "\n",
    "    Args:\n",
    "        dir (Path): The directory path.\n",
    "        file_name (str): The file name to create.\n",
    "    \"\"\"\n",
    "    dir = Path(dir_name)\n",
    "    file_path = dir.joinpath(file_name)\n",
    "    if not dir.exists():\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "    elif not file_path.is_file():\n",
    "        file_path.touch()\n",
    "\n",
    "\n",
    "def check_files_exist(directory: str, file_names: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if files exist in the given directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory path.\n",
    "        file_names (List[str]): List of file names to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: If all of the files exist in the directory.\n",
    "    \"\"\"\n",
    "    return all(Path(directory).joinpath(name).is_file() for name in file_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, nbits: int, embedding_size: int, seed: int):\n",
    "        self.nbits = nbits\n",
    "        self.seed = seed\n",
    "        self.plane_norms = self._generate_plane_norms(embedding_size)\n",
    "\n",
    "    def _generate_plane_norms(self, embedding_size: int) -> np.ndarray:\n",
    "        rng = np.random.RandomState(self.seed)\n",
    "        return rng.rand(self.nbits, embedding_size) - 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(texts: List[str], model: str) -> np.ndarray:\n",
    "        client = OpenAI()\n",
    "        processed_texts = [\n",
    "            text.replace(\"\\n\", \" \").replace(\"<br />\", \" \") for text in texts\n",
    "        ]\n",
    "        embeddings = client.embeddings.create(input=processed_texts, model=model).data\n",
    "        return np.array([embedding.embedding for embedding in embeddings])\n",
    "\n",
    "    def hash_vector(self, v: np.ndarray) -> List[str]:\n",
    "        v_dots = np.dot(v, self.plane_norms.T) > 0\n",
    "        return [\"\".join(str(int(i)) for i in v_dot) for v_dot in v_dots]\n",
    "\n",
    "    @staticmethod\n",
    "    def bucket_hashes(v: List[str]) -> Dict[str, List[int]]:\n",
    "        buckets = {}\n",
    "        for idx, hash_str in enumerate(v):\n",
    "            buckets.setdefault(hash_str, []).append(idx)\n",
    "        return buckets\n",
    "\n",
    "    @staticmethod\n",
    "    def hashes_to_df(v: List[str], col1: str, col2: str) -> pl.DataFrame:\n",
    "        buckets = {}\n",
    "        for i, hash_str in enumerate(v):\n",
    "            buckets.setdefault(hash_str, []).append(i)\n",
    "\n",
    "        buckets_df = pl.from_dict(\n",
    "            {col1: list(buckets.keys()), col2: list(buckets.values())}\n",
    "        )\n",
    "        return buckets_df\n",
    "\n",
    "    @staticmethod\n",
    "    def write_buckets_to_csv(\n",
    "        buckets: Dict[str, List[int]], col1: str, col2: str, file_path: str\n",
    "    ):\n",
    "        # Open the file in write mode\n",
    "        with open(file_path, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([col1, col2])\n",
    "\n",
    "            for key, value in buckets.items():\n",
    "                list_as_string = str(value).strip(\"[]\")\n",
    "                writer.writerow([key, list_as_string])\n",
    "\n",
    "    @staticmethod\n",
    "    def hamming_distance(str1: str, str2: str) -> int:\n",
    "        if len(str1) != len(str2):\n",
    "            raise ValueError(\"Strings must be of equal length\")\n",
    "        return sum(char1 != char2 for char1, char2 in zip(str1, str2))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_text_idx(hamming_distances: List[int]) -> int:\n",
    "        if len(hamming_distances) == 0:\n",
    "            raise ValueError(\"No hamming distances found\")\n",
    "        return int(np.argmin(hamming_distances))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Preprocess the data and generate the hash buckets for the text reviews.<br/>\n",
    "Store the buckets for different nbits into different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Embeddings, Hashes, Buckets to CSV files...\n",
      "\n",
      "\tfor nbits = 8 ✅\n",
      "\n",
      "\tfor nbits = 16 ✅\n",
      "\n",
      "\tfor nbits = 32 ✅\n",
      "\n",
      "\tfor nbits = 64 ✅\n",
      "\n",
      "\tfor nbits = 128 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"output\"\n",
    "file_name = \"preprocessed_data.csv\"\n",
    "\n",
    "# load data\n",
    "df = pl.read_csv(data_file)\n",
    "reviews = df.get_column(\"Text\").to_list()\n",
    "\n",
    "# Generate embeddings for each review\n",
    "embeddings = LSH.get_embedding(reviews, model)\n",
    "\n",
    "reviews_updated = [\n",
    "    review.replace(\"\\n\", \" \").replace(\"<br />\", \" \") for review in reviews\n",
    "]\n",
    "\n",
    "# Create DataFrame with updated reviews and embeddings\n",
    "df2 = pl.DataFrame(\n",
    "    {\n",
    "        \"Text\": reviews_updated,\n",
    "        \"Embedding\": [str(embedding) for embedding in embeddings.tolist()],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Writing Embeddings, Hashes, Buckets to CSV files...\\n\")\n",
    "for nbits in [8, 16, 32, 64, 128]:\n",
    "    # Create LSH instance\n",
    "    lsh = LSH(nbits=nbits, seed=seed, embedding_size=embedding_size)\n",
    "\n",
    "    # Hash each embeddings into a hash code. Hence, a list of hash codes is returned\n",
    "    hashes = lsh.hash_vector(embeddings)\n",
    "\n",
    "    # Add LSH hashes corresponding to the embeddings to the df2 DataFrame\n",
    "    df2.insert_column(len(df2.columns), pl.Series(f\"Hash {nbits}-bit\", hashes))\n",
    "\n",
    "    # Hashes to buckets\n",
    "    buckets = lsh.bucket_hashes(hashes)\n",
    "\n",
    "    # Define the path for the directory and ensure the file\n",
    "    bucket_file_name = f\"buckets_{nbits}bit.csv\"\n",
    "    ensure_file_exists(dir_name, f\"buckets_{nbits}bit.csv\")\n",
    "\n",
    "    # write to CSV\n",
    "    lsh.write_buckets_to_csv(\n",
    "        buckets, \"Text Hash\", \"Text Indices\", f\"{dir_name}/{bucket_file_name}\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\tfor nbits = {nbits} ✅\\n\")\n",
    "\n",
    "# Define the path for the directory and the file\n",
    "ensure_file_exists(dir_name, file_name)\n",
    "\n",
    "\"\"\" Save embeddings + LSH to CSV, linked with source sample \"\"\"\n",
    "df2.write_csv(f\"{dir_name}/{file_name}\", separator=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of similar text\n",
    "\n",
    "Here, parsing a similar text (to 1st food review, say) and see if it is falling into the expected bucket or not.\n",
    "\n",
    "Suppose the 1st review is slightly modified from:\n",
    "\n",
    "```text\n",
    "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
    "```\n",
    "\n",
    "to:\n",
    "\n",
    "```text\n",
    "I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====For nbits = 8======\n",
      "For a given text: \n",
      "\"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\", \n",
      "it's computed hash is '10010010'.\n",
      "😊 Falls into a bucket with HD == 0.\n",
      "The bucket contains texts at indices: 3, 31, 35, 147, 227, 248, 284, 285, 354, 355, 425, 436, 442, 457, 462, 506, 509, 518, 519, 521, 526, 532, 546, 558, 580, 608, 616, 627, 653, 690, 698, 716, 725, 835, 970.\n",
      "\n",
      "=====For nbits = 16======\n",
      "For a given text: \n",
      "\"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\", \n",
      "it's computed hash is '1001001011101011'.\n",
      "😟 Falls into closest bucket with HD != 0,\n",
      "when traversed from left --> right.\n",
      "The bucket contains texts at indices: 0.\n",
      "\n",
      "=====For nbits = 32======\n",
      "For a given text: \n",
      "\"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\", \n",
      "it's computed hash is '10010010111010110011010100011011'.\n",
      "😟 Falls into closest bucket with HD != 0,\n",
      "when traversed from left --> right.\n",
      "The bucket contains texts at indices: 0.\n",
      "\n",
      "=====For nbits = 64======\n",
      "For a given text: \n",
      "\"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\", \n",
      "it's computed hash is '1001001011101011001101010001101111001111001100100111001011000011'.\n",
      "😟 Falls into closest bucket with HD != 0,\n",
      "when traversed from left --> right.\n",
      "The bucket contains texts at indices: 0.\n",
      "\n",
      "=====For nbits = 128======\n",
      "For a given text: \n",
      "\"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\", \n",
      "it's computed hash is '10010010111010110011010100011011110011110011001001110010110000110011110111001010100001110111011011110100101100011000010111101011'.\n",
      "😟 Falls into closest bucket with HD != 0,\n",
      "when traversed from left --> right.\n",
      "The bucket contains texts at indices: 0.\n"
     ]
    }
   ],
   "source": [
    "# slightly modified 1st review from the dataset\n",
    "query = \"I have bought many of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells good. My Labrador is finicky and she likes this product better than  most.\"\n",
    "\n",
    "required_files = [f\"buckets_{nbits}bit.csv\" for nbits in [8, 16, 32, 64, 128]] + [\n",
    "    \"preprocessed_data.csv\"\n",
    "]\n",
    "if not check_files_exist(\"output\", required_files):\n",
    "    raise ValueError(\"Please run `preprocessing.py` first\")\n",
    "\n",
    "for nbits in [8, 16, 32, 64, 128]:\n",
    "    print(f\"\\n=====For nbits = {nbits}======\")\n",
    "\n",
    "    # instantiate LSH\n",
    "    lsh = LSH(nbits=nbits, embedding_size=embedding_size, seed=seed)\n",
    "\n",
    "    # get hash of a query text\n",
    "    query_hash = lsh.hash_vector(lsh.get_embedding([query], model))[0]\n",
    "    print(\n",
    "        f\"For a given text: \\n\\\"{query}\\\", \\nit's computed hash is '{query_hash}'.\"\n",
    "    )\n",
    "\n",
    "    # load data\n",
    "    df = pl.read_csv(\n",
    "        f\"output/buckets_{nbits}bit.csv\", dtypes={\"Text Hash\": pl.String}\n",
    "    )\n",
    "    bucket_hashes = df.get_column(\"Text Hash\").to_list()\n",
    "    bucket_indices = df.get_column(\"Text Indices\").to_list()\n",
    "\n",
    "    # get hamming distances between the query and each bucket key\n",
    "    hamming_distances = [\n",
    "        lsh.hamming_distance(query_hash, hash_str) for hash_str in bucket_hashes\n",
    "    ]\n",
    "\n",
    "    # HD: Hamming distance\n",
    "    if 0 in hamming_distances:\n",
    "        print(\"😊 Falls into a bucket with HD == 0.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"😟 Falls into closest bucket with HD != 0,\\nwhen traversed from left --> right.\"\n",
    "        )\n",
    "    print(\n",
    "        f\"The bucket contains texts at indices: {bucket_indices[lsh.get_text_idx(hamming_distances)]}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results, the query text does fall into the bucket with HD = 0, but the bucket does not contain the original text i.e. text at index-0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
