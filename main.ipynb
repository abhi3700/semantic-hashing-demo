{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Hashing Demo\n",
    "\n",
    "## Description\n",
    "\n",
    "The objective is to identify texts that are semantically similar using Locality Sensitive Hashing (LSH) or what we refer to as \"semantic hashing\". In order to test the effectiveness of semantic hashing, texts are generated using [Marvin](https://www.askmarvin.ai/) through their `marvin` Python package. The dataset includes pairs of semantically similar paragraphs. \n",
    "\n",
    "## Process\n",
    "\n",
    "All paragraphs are first tokenized and then embedded using a pre-trained transformer model.\n",
    "Subsequently, for any given text, a semantic hash is computed. This process involves converting a large embedding vector (a numerical representation of text) into a few bits (representing the number of hyperplanes), akin to a hash code. For example, the phrase \"The food was very delicious\" could be represented as `1011` in a system with 4 hyperplanes. Finally, texts with identical hashes are grouped into multiple buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import marvin\n",
    "from pydantic import BaseModel, Field\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import sys\n",
    "\n",
    "# Add the 'src' directory to the Python path to find the required modules\n",
    "src_path = Path(\"main.ipynb\").parent.resolve() / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from semantic_hashing_demo.utils import ensure_file_exists, check_files_exist\n",
    "from semantic_hashing_demo.lsh import LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "All the parameters (constants mostly) are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the input data for the semantic hashing demo.\n",
    "\"\"\"\n",
    "\n",
    "# no. of hyperplanes\n",
    "nbits = 8\n",
    "\n",
    "\n",
    "# data file\n",
    "# NOTE: This data file has around 570k text reviews (of types: single line, paragraph).\n",
    "# So, parse accordingly depending on the computational resources for bucketing.\n",
    "data_file = \"./data/fine_food_reviews_1k.csv\"\n",
    "preprocessed_data_file = \"./output/preprocessed_data.csv\"\n",
    "generated_data_file = \"./data/paragraphs.csv\"\n",
    "\n",
    "# no. of text samples\n",
    "n = 20\n",
    "\n",
    "# seed for hyperplane generation\n",
    "seed = 2254  # subspace address format prefix\n",
    "\n",
    "# embedding model\n",
    "model, embedding_size = \"text-embedding-3-small\", int(1536)\n",
    "# model, embedding_size = \"text-embedding-3-large\", int(3072)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data (Optional)\n",
    "\n",
    "Optionally, generate new `paragraphs.csv` data using `marvin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphData(BaseModel):\n",
    "    original: str = Field(description=\"The original paragraph\")\n",
    "    very_similar: str = Field(\n",
    "        description=\"A paragraph that is almost identical to the original paragraph with only a couple of words changed\"\n",
    "    )\n",
    "\n",
    "def generate_data():\n",
    "    print(\"generating data\")\n",
    "    new_data = marvin.generate(\n",
    "        n=4,\n",
    "        target=ParagraphData,\n",
    "        instructions=\"generate paragraphs for comparison testing. the paragraphs should be almost identical, with only a few words changed. each paragraph should be at least 100 words long.\",\n",
    "    )\n",
    "    return new_data\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "# Number of parallel calls you want to make\n",
    "num_parallel_calls = 10\n",
    "\n",
    "# Use ThreadPoolExecutor to execute calls in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_parallel_calls) as executor:\n",
    "    # Submit all your generate calls to the executor\n",
    "    future_to_generate = {\n",
    "        executor.submit(generate_data) for _ in range(num_parallel_calls)\n",
    "    }\n",
    "\n",
    "    # Collect the results as they are completed\n",
    "    for future in as_completed(future_to_generate):\n",
    "        try:\n",
    "            data.extend(future.result())\n",
    "        except Exception as exc:\n",
    "            print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "data_dicts = [d.dict() for d in data]\n",
    "df = pl.DataFrame(data_dicts)\n",
    "df.write_csv(\"./data/paragraphs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Generated data\n",
    "\n",
    "Load the generated data and save LSH codes for both source and variants with increasing nbits i.e. 8, 16, 32, 64, 128 hyperplanes.\n",
    "And then plot HD matrix between all sources and variants for each nbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"output\"\n",
    "file_name = \"paragraphs_processed.csv\"\n",
    "\n",
    "# load data\n",
    "df = pl.read_csv(\"data/paragraphs.csv\")\n",
    "source_texts = df.get_column(\"original\").to_list()\n",
    "variants_texts = df.get_column(\"very_similar\").to_list()\n",
    "\n",
    "# get embeddings for source and variants\n",
    "source_embeddings = LSH.get_embedding(source_texts, model)\n",
    "variant_embeddings = LSH.get_embedding(variants_texts, model)\n",
    "\n",
    "# Create DataFrame with embeddings of source and variants\n",
    "df2 = pl.DataFrame(\n",
    "    {\n",
    "        \"Source\": source_texts,\n",
    "        \"Variant\": variants_texts,\n",
    "        \"Source Embedding\": [\n",
    "            str(embedding) for embedding in source_embeddings.tolist()\n",
    "        ],\n",
    "        \"Variant Embedding\": [\n",
    "            str(embedding) for embedding in variant_embeddings.tolist()\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Saving Embeddings, LSH codes & HD matrix...\\n\")\n",
    "for nbits in [8, 16, 32, 64, 128]:\n",
    "    print(f\"\\tfor nbits = {nbits}:\")\n",
    "    # Create LSH instance\n",
    "    lsh = LSH(nbits=nbits, seed=seed, embedding_size=embedding_size)\n",
    "\n",
    "    hashes_source = lsh.hash_vector(source_embeddings)\n",
    "    hashes_variant = lsh.hash_vector(variant_embeddings)\n",
    "\n",
    "    # Add LSH hashes corresponding to the embeddings to the df2 DataFrame\n",
    "    df2.insert_column(\n",
    "        len(df2.columns), pl.Series(f\"Source Hash {nbits}-bit\", hashes_source)\n",
    "    )\n",
    "    df2.insert_column(\n",
    "        len(df2.columns), pl.Series(f\"Variant Hash {nbits}-bit\", hashes_variant)\n",
    "    )\n",
    "\n",
    "    hamming_distances = []\n",
    "    # calculate HD matrix\n",
    "    for hash_variant in hashes_variant:\n",
    "        hamming_distances.append(\n",
    "            [\n",
    "                lsh.hamming_distance(hash_source, hash_variant)\n",
    "                for hash_source in hashes_source\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # generate a plot for nbits hyperplanes\n",
    "    fig = px.imshow(hamming_distances)\n",
    "    fig.show()\n",
    "    py.offline.plot(fig, filename=f\"output/plot_matrix_{nbits}.html\", auto_open=False)\n",
    "\n",
    "# Ensure the file in desired path\n",
    "ensure_file_exists(dir_name, file_name)\n",
    "\n",
    "\"\"\" Save embeddings + LSH to CSV, linked with source sample \"\"\"\n",
    "df2.write_csv(f\"{dir_name}/{file_name}\", separator=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
